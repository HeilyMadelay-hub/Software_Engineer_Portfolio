IDEA ORIGINAL:
1. Objetivo 
Es un algoritmo de reconocimiento de gestos dinámicos en tiempo real que transforma movimiento humano en una firma matemática temporal, la compara con patrones pregrabados y genera salida semántica (palabras y frases).
Propiedades clave:
Independiente del lenguaje y framework
Independiente del modelo (con o sin IA)
Válido en tiempo real
Matemáticamente interpretable

2. Representación matemática del frame (preprocesamiento)
Para cada frame (t), se extraen los landmarks de la mano.
1 mano
[
\mathbf{f}t =
(x{1,t}, y_{1,t}, \ldots, x_{21,t}, y_{21,t})
\in \mathbb{R}^{42}
]
2 manos
[
\mathbf{f}_t \in \mathbb{R}^{84}
]

2.1 Normalización de escala 
Para garantizar invarianza a escala, se normaliza el frame:
[
\tilde{\mathbf{f}}_t = \frac{\mathbf{f}_t}{\text{scale}_t}
]
donde scale_t puede ser:
distancia muñeca → MCP del dedo medio
o diagonal del bounding box de la mano
El sistema queda:
invariante a traslación
invariante a escala
robusto a profundidad

3. Vector de desplazamiento (dinámica del gesto)
El movimiento se modela mediante diferencias entre frames consecutivos:
[
\mathbf{d}_t = \tilde{\mathbf{f}}t - \tilde{\mathbf{f}}{t-1}
]
Dimensión:
( \mathbb{R}^{42} ) (1 mano)
( \mathbb{R}^{84} ) (2 manos)
Cada par ((dx_i, dy_i)) representa velocidad y dirección del landmark (i).

4. Buffer temporal
Se mantiene un buffer deslizante de tamaño (N):
[
B = {\mathbf{d}_{t-N+1}, \ldots, \mathbf{d}_t}
]
Este buffer representa una ventana temporal del gesto.

5. Huella promedio del gesto
La huella dinámica del gesto se define como:
[
\mathbf{P}d = \frac{1}{N} \sum{i=1}^{N} \mathbf{d}_i
]
Interpretación:
dirección dominante del movimiento
reducción de ruido y jitter
estabilidad temporal

6. Energía del movimiento
Para gestos oscilatorios o con cambios de dirección:
[
E = \frac{1}{N} \sum_{i=1}^{N} |\mathbf{d}_i|^2
]
Uso:
filtro adicional
refuerzo de decisión
criterio de estabilidad
(opcional, no obligatorio)

7. Firma matemática del gesto (offline)
Para cada gesto base (i), con (M) ejemplos:
[
\mathbf{F}{d,i} = \frac{1}{M} \sum{j=1}^{M} \mathbf{d}_j
]
Variabilidad del gesto
[
\sigma_i =
\sqrt{
\frac{1}{M}
\sum_{j=1}^{M}
|\mathbf{d}j - \mathbf{F}{d,i}|^2
}
]

8. Umbral robusto de detección 
Se utiliza un umbral acotado y estable:
[
\text{umbral}_i = \exp(-k \cdot \sigma_i)
]
Propiedades:
(0 < \text{umbral}_i \le 1)
estable numéricamente
independiente de la dimensión

9. Medida de similitud
Se emplea similitud coseno:
[
\operatorname{sim}(\mathbf{P}d, \mathbf{F}{d,i}) =
\frac{\mathbf{P}d \cdot \mathbf{F}{d,i}}
{|\mathbf{P}d| , |\mathbf{F}{d,i}|}
]
Ventajas:
ignora magnitud absoluta
compara dirección del movimiento
ideal en espacios de alta dimensión

10. Criterio de detección del gesto
Un gesto (i) se detecta si:
[
\operatorname{sim}(\mathbf{P}d, \mathbf{F}{d,i})
\ge
\text{umbral}_i
]
Salida:
etiqueta semántica (L_i)

11. Máquina de Estados Finita (tiempo real)
Estados
S_Inactivo: no hay mano detectada
S_Captura: se llena el buffer
S_Detectado: gesto reconocido
S_Pausa: ausencia de mano durante (\Delta t)
Transiciones
Inactivo → Captura (mano detectada)
Captura → Detectado (sim ≥ umbral)
Detectado → Captura (movimiento continuo)
Cualquiera → Pausa (no detección por (\Delta t))

12. Acumulación semántica (palabras)
Cada detección añade una etiqueta:
[
\text{Secuencia} = [L_1, L_2, \ldots, L_m]
]
Opciones:
emitir voz inmediata por palabra
o esperar a frase completa

13. Evaluación de frase (pausa prolongada)
Al entrar en S_Pausa, se evalúa la secuencia observada (S_{obs}) frente a un conjunto de frases (\mathcal{S}).
Métodos:
coincidencia exacta
distancia de edición (\le \tau)
modelo probabilístico

14. Emisión de salida
Si hay coincidencia → TTS de frase completa
Si no → fallback palabra por palabra o mensaje genérico

15. Ciclo completo del sistema
[
G_1 \rightarrow L_1 \rightarrow G_2 \rightarrow L_2 \rightarrow \cdots
\xrightarrow{\Delta}
\text{Evaluación}([L_1,\ldots,L_m])
\rightarrow \text{Voz}
]

16. Propiedades finales
✔ Invariante a traslación
✔ Invariante a escala
✔ Independiente de dimensión (42 / 84)
✔ Robusto al ruido
✔ Tiempo real
✔ Sin dependencia de IA pesada

17. Sustitución por LSTM

Solo se reemplaza la función de decisión:
[
\mathbf{P}_d
;\longrightarrow;
\text{LSTM}(B)
;\longrightarrow;
\text{clase}
]
El resto del sistema permanece idéntico.

¿Como lo hemos implementado?

FASE 1 — Captura de keypoints

Se capturan los landmarks de la mano utilizando MediaPipe.

Cada frame genera un array de coordenadas double[] keypoints (21 puntos por mano, x, y, z → 63 valores; dos manos → 126 valores).

Los datos se envían tal cual al backend, sin normalización ni procesamiento previo.

El Hub actúa únicamente como transportador de datos, asegurando separación de responsabilidades.

Actualmente, esta fase está completamente implementada.

FASE 2 — Gestión de firmas

GestureSignatureService se encarga de:

Cargar los patrones de gestos desde JSON ubicados en wwwroot/gestures.

Mantenerlos en memoria.

Permitir su consulta por nombre o listarlos todos.

No se realiza detección aquí; solo se preparan y organizan los datos.

Esta fase también está implementada y funciona como base para la comparación matemática posterior.

FASE 3 — Detección matemática (Core)

GestureDetectorService recibe los keypoints.

Calcula la similitud coseno con cada firma (CalculateCosineSimilarity).

Aplica un umbral precalculado para determinar si el gesto coincide.

Devuelve un DetectionResult con:

Detected → si se reconoció un gesto.

GestureName → nombre del gesto detectado.

Similarity → nivel de similitud calculado.

Threshold → umbral del gesto.

Esta fase corresponde al núcleo matemático del sistema y ya está completamente implementada. Incluye los pasos 8–10 del algoritmo, pero todavía no incluye:

Preprocesamiento dinámico de frames (diferencias entre frames)

Buffer temporal de gestos

Evaluación de secuencias o frases

Estados (S_Captura, S_Detectado, S_Pausa)

FASE 4 — Orquestación

CameraHub recibe los keypoints crudos desde el frontend.

Invoca el GestureDetectorService y envía al cliente únicamente la información semántica (GestureDetected).

Se aplica el Principio de Inversión de Dependencias: el Hub depende de la interfaz del servicio, no de su implementación concreta, garantizando modularidad y facilidad de pruebas.

Esta fase ya está implementada.

FASE 5 — Experiencia de usuario y buffer temporal (opcional)

En el frontend, mediante un buffer temporal (ej. gesture-buffer.js), se acumulan los gestos para:

Detectar secuencias o frases.

Aplicar estados (S_Captura, S_Detectado, S_Pausa).

Evaluar frases completas y generar TTS.

Es opcional porque no forma parte del núcleo matemático ni del algoritmo de detección de gestos. Su objetivo principal es mejorar la experiencia de usuario y la interpretación temporal de los gestos:

Buffer temporal y estados: permite reconocer secuencias, pero el sistema puede funcionar sin esto detectando gestos individuales.

Procesamiento en frontend: se hace fuera del backend; el núcleo de detección ya devuelve resultados semánticos correctos por sí mismo.

Flexibilidad: se puede prescindir de esta fase en implementaciones mínimas o de prueba sin afectar la exactitud matemática.

Actualmente, esta fase no está implementada; se puede añadir después para mejorar UX y manejo de frases.

Resumen de implementación respecto al algoritmo matemático

Representación del frame: se capturan landmarks de 1 o 2 manos (R^42 o R^84).

Normalización de escala: aún no implementada; se podrían normalizar coordenadas para invariancia a tamaño y traslación.

Vector de desplazamiento y buffer temporal: pendiente; se usaría para dinámica de gestos.

Huella promedio del gesto: parcialmente, mediante las firmas precalculadas (FirmaPromedio).

Umbral robusto y similitud coseno: implementado en GestureDetectorService.

Criterio de detección: implementado (sim ≥ umbral).

Estados y acumulación semántica de secuencias: opcional, aún no implementado.

Evaluación de frases y salida TTS: opcional, aún no implementado.

Propiedades finales: invariancia parcial a traslación y escala (pendiente de normalización), robustez al ruido, independencia de IA pesada.

Resumen ejecutivo

El sistema aplica un enfoque escalonado y matemáticamente riguroso: captura keypoints crudos, gestiona firmas pregrabadas, calcula similitud coseno para reconocer gestos y devuelve resultados semánticos de manera desacoplada. Cada fase cumple un rol específico, garantizando robustez, independencia de la interfaz y portabilidad del sistema. Las fases opcionales permiten mejorar UX y manejar secuencias sin alterar la lógica central de detección.
